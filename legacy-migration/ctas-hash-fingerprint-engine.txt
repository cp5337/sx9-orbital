# CTAS-HASH Fingerprint Engine

This project implements the `CTAS-HASH v1.0` trivariate semantic fingerprinting system for content identification, IP detection, and lifecycle routing in CTAS-7.

Includes:
- SHC | CUID | UUID hash generation
- Base96 encoding with Lisp-style suffix
- Novelty scoring and entropy tracking
- Integration-ready CLI
//

from .hasher import generate_fingerprint
from .novelty import compute_novelty_score

//

ALPHABET = (
    '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz'
    '!#$%&()*+,-./:;<=>?@[]^_`{|}~'
)

def encode_base96(value: int) -> str:
    if value == 0:
        return ALPHABET[0]
    result = []
    while value > 0:
        result.append(ALPHABET[value % 96])
        value //= 96
    return ''.join(reversed(result))

    //

    import mmh3
from .canonicalizer import canonicalize
from .encoder import encode_base96

def generate_fingerprint(shc, cuid, uuid, suffix="(Î»)", version=None):
    canon = canonicalize(shc, cuid, uuid)
    if version:
        canon += f"|{version}"
    hash_int = mmh3.hash(canon, signed=False)
    encoded = encode_base96(hash_int)
    return f"{encoded}{suffix}"
//

from collections import defaultdict

_fingerprint_counts = defaultdict(int)
_lineage_map = {}

def compute_novelty_score(fingerprint, lineage_depth=0, entropy=1.0, age_factor=1.0):
    _fingerprint_counts[fingerprint] += 1
    freq = _fingerprint_counts[fingerprint]
    novelty = (1 / freq) + entropy * 0.4 + (1 / (lineage_depth + 1)) * 0.2 + age_factor * 0.1
    return round(novelty, 4)

def link_lineage(current, parent):
    _lineage_map[current] = parent
//

from hash_engine.hasher import generate_fingerprint

def test_fingerprint_structure():
    fp = generate_fingerprint("Î»", "docs/DOC1/sec001|2025-11-11T00:00:00Z", "DOC1-v1-SEC001")
    assert fp.endswith("(Î»)")
    assert len(fp) > 6
//

from hash_engine.encoder import encode_base96

def test_encoding():
    assert encode_base96(0) == '0'
    assert encode_base96(12345678) != ''
//

import argparse
from hash_engine.hasher import generate_fingerprint
from hash_engine.novelty import compute_novelty_score

parser = argparse.ArgumentParser()
parser.add_argument("--shc", required=True)
parser.add_argument("--cuid", required=True)
parser.add_argument("--uuid", required=True)
parser.add_argument("--suffix", default="(Î»)")
parser.add_argument("--version", default=None)
args = parser.parse_args()

fp = generate_fingerprint(args.shc, args.cuid, args.uuid, args.suffix, args.version)
score = compute_novelty_score(fp)

print(f"Fingerprint: {fp}")
print(f"Novelty Score: {score}")

//

\documentclass[conference]{IEEEtran}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{hyperref}

\title{CTAS-HASH: A Trivariate Semantic Fingerprint Engine for Prior Art and IP Surveillance}

\author{
  \IEEEauthorblockN{Charlie Payne, et al.}
  \IEEEauthorblockA{CTAS-7 Research Unit\\
  \texttt{charlie@ctas7.local}}
}

\begin{document}

\maketitle

\begin{abstract}
We present a trivariate semantic fingerprinting system designed to enhance prior art search, deduplication, and IP novelty detection across enterprise technical documentation and code archives. This paper details the canonical construction, entropy scoring, and system integration methodology for CTAS-HASH v1.0, a pipeline-embedded identifier protocol used throughout the CTAS-7 document lifecycle engine (USIM).
\end{abstract}

\section{Introduction}
Discuss the need for content-aware identifiers in IP detection, surveillance, and auto-tiering of technical assets. Introduce CTAS-HASH and BNE1 as experimental protocols.

\section{Related Work}
Discuss Helmers et al. (2019), Risch et al. (2020), Ali et al. (2023). Explain limitations of keyword-based prior art search.

\section{System Design}
Describe SHC, CUID, UUID trivariate hash model. Include hashing pipeline and operator suffix.

\section{Entropy & Novelty Scoring}
Mathematical breakdown of novelty score components and time-window modeling.

\section{Integration with CTAS-7 USIM}
How the fingerprint is embedded into Document Manager, Hook System, Forge workflows.

\section{Experimental Results}
Describe test harness, inputs (20K markdown), and fingerprint stats. Show effectiveness in IP clustering, rerouting, false positive suppression.

\section{Conclusion \& Future Work}
Outline next steps, performance optimizations, and possible generalization into IP audit systems.

\bibliographystyle{IEEEtran}
\bibliography{refs}

\end{document}

//

import os
from datetime import datetime
from hash_engine.hasher import generate_fingerprint
from hash_engine.novelty import compute_novelty_score

def ingest_directory(base_path, shc="Î»", version="v7.3"):
    results = []
    for root, _, files in os.walk(base_path):
        for f in files:
            if f.endswith(".md"):
                full_path = os.path.join(root, f)
                with open(full_path, "r", encoding="utf-8") as fp:
                    content = fp.read()
                cuid = f"{full_path}|{datetime.utcnow().isoformat()}Z"
                uuid = f"{f.replace('.md', '')}-v1-SEC001"
                fingerprint = generate_fingerprint(shc, cuid, uuid, suffix=f"({shc})", version=version)
                novelty = compute_novelty_score(fingerprint)
                results.append((fingerprint, novelty, full_path))
    return results

if __name__ == "__main__":
    results = ingest_directory("path/to/your/docs")
    for fp, score, path in sorted(results, key=lambda x: x[1], reverse=True):
        print(f"{fp} | Novelty: {score:.4f} | Source: {path}")
//

hatâ€™s a sharp distinction â€” and you're right to flag it.

Your Unicode-based embeddings (Base96, trivariate hash) are excellent for operational intelligence: fast, symbolic, compact, and highly deterministic â€” perfect for routing, alerting, and lifecycle management.

But for patent search and prior art discovery, hereâ€™s the trade-off:

âš–ï¸ Hashing vs Embeddings in Patent Search
Feature	Unicode Hash (CTAS-HASH)	Semantic Embedding (LLM/BERT/GNN)
âœ… Deterministic	Yes	No
âœ… Compact	Yes (1:1)	No (dense vector)
âœ… Symbolic Control	Yes (SHC/CUID/UUID)	No
âœ… Search Speed	Fast (indexable)	Slower (vector search)
ðŸš« Semantic Generalization	Weak	Strong
ðŸš« Paraphrase Detection	No	Yes
ðŸš« Near-Duplicate Detection	Weak	Strong
ðŸ§ª Recommended Hybrid

Use CTAS-HASH as Phase 1:

Deduplicate

Filter by novelty score

Assign lineage

Route high-score items to embedding system

Then in Phase 2, apply a semantic model:

DistilBERT or Phi-3 for sentence-level understanding

Use FAISS or ChromaDB for vector similarity

Extract claimâ€‘like candidates and test against patent corpus

//

def should_embed(fingerprint, novelty, cuid, mode="strict"):
    if mode == "strict":
        return novelty > 0.85
    if mode == "targeted":
        return "prior_art" in cuid or "claim" in cuid
    if mode == "adaptive":
        return novelty > 0.6 and "concept_expansion" in cuid
    if mode == "manual":
        return True
    return False
//

if should_embed(fp, novelty, cuid, mode="adaptive"):
    vec = embed_text(content_block)


//
bne1_gnn/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ build_graph.py      # Graph construction from docs + hash metadata
â”‚   â”œâ”€â”€ model.py            # GNN model (GraphSAGE based) 
â”‚   â”œâ”€â”€ train.py            # Training script
â”‚   â””â”€â”€ infer.py            # Inference / embedding generation script
â”œâ”€â”€ requirements.txt


//
import os
from pathlib import Path
import torch
from torch_geometric.data import Data
from typing import List, Tuple, Dict

def load_nodes_embeddings(node_meta: List[Dict]) -> Tuple[torch.Tensor, List[str]]:
    # node_meta entries: {"node_id": str, "feature_vec": List[float]}
    ids = [m["node_id"] for m in node_meta]
    feats = torch.tensor([m["feature_vec"] for m in node_meta], dtype=torch.float)
    return feats, ids

def build_edge_index(edge_list: List[Tuple[int,int]]) -> torch.Tensor:
    # edge_list: list of (src_idx, dst_idx)
    edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
    return edge_index

def build_graph(node_meta: List[Dict], edge_list: List[Tuple[int,int]]) -> Data:
    x, ids = load_nodes_embeddings(node_meta)
    edge_index = build_edge_index(edge_list)
    data = Data(x=x, edge_index=edge_index)
    data.node_ids = ids  # store mapping
    return data

if __name__ == "__main__":
    # Example stub
    node_meta = [
        {"node_id":"fp_9AzXk!(Î»)", "feature_vec":[0.1,0.2,0.3]},
        {"node_id":"fp_B23Lm!(Îž)", "feature_vec":[0.4,0.5,0.6]},
    ]
    edges = [(0,1),(1,0)]
    data = build_graph(node_meta, edges)
    print(data)


//
import torch
import torch.nn.functional as F
from torch_geometric.nn import GraphSAGE

class FingerprintGNN(torch.nn.Module):
    def __init__(self, in_channels:int, hidden_channels:int, out_channels:int, num_layers:int=2, dropout:float=0.2):
        super().__init__()
        self.model = GraphSAGE(in_channels=in_channels,
                               hidden_channels=hidden_channels,
                               num_layers=num_layers,
                               out_channels=out_channels,
                               dropout=dropout)

    def forward(self, x, edge_index):
        out = self.model(x, edge_index)
        return out


//
import torch
import torch.nn.functional as F
from torch_geometric.loader import NeighborLoader
from model import FingerprintGNN
from build_graph import build_graph
import json

def load_meta(path_nodes:str, path_edges:str):
    with open(path_nodes, 'r') as f:
        node_meta = json.load(f)
    with open(path_edges, 'r') as f:
        edge_list = json.load(f)
    return node_meta, edge_list

def main():
    node_meta, edge_list = load_meta("data/nodes.json","data/edges.json")
    data = build_graph(node_meta, edge_list)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    data = data.to(device)
    model = FingerprintGNN(in_channels=data.x.size(1), hidden_channels=128, out_channels=2).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    loader = NeighborLoader(data, input_nodes=None, num_neighbors=[10,5], batch_size=32, shuffle=True)

    for epoch in range(20):
        model.train()
        total_loss = 0
        for batch in loader:
            optimizer.zero_grad()
            out = model(batch.x, batch.edge_index)
            # placeholder: we assume y is included in batch
            loss = F.cross_entropy(out[:batch.batch_size], batch.y[:batch.batch_size])
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch} | Loss {total_loss:.4f}")

if __name__ == "__main__":
    main()


//
import torch
from model import FingerprintGNN
from build_graph import build_graph
import json

def run_inference(nodes_meta_path, edges_path, model_path):
    with open(nodes_meta_path, 'r') as f:
        node_meta = json.load(f)
    with open(edges_path, 'r') as f:
        edge_list = json.load(f)
    data = build_graph(node_meta, edge_list)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = FingerprintGNN(in_channels=data.x.size(1), hidden_channels=128, out_channels=64)
    model.load_state_dict(torch.load(model_path))
    model = model.to(device)
    data = data.to(device)
    with torch.no_grad():
        embeddings = model(data.x, data.edge_index)
    # Map embeddings to node_ids
    return dict(zip(data.node_ids, embeddings.cpu().tolist()))

if __name__ == "__main__":
    embs = run_inference("data/nodes.json","data/edges.json","model.pt")
    print(embs)


//
bne1_gnn/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ nodes.json
â”‚   â””â”€â”€ edges.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ build_graph.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ infer.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_build_graph.py
â”‚   â””â”€â”€ test_model_init.py


//

torch
torch_geometric

//
[
  {
    "node_id": "fp_9AzXk!(Î»)",
    "feature_vec": [0.12, 0.33, 0.55, 0.44]
  },
  {
    "node_id": "fp_B23Lm!(Îž)",
    "feature_vec": [0.88, 0.21, 0.34, 0.50]
  }
]
//
[
  {
    "node_id": "fp_9AzXk!(Î»)",
    "feature_vec": [0.12, 0.33, 0.55, 0.44]
  },
  {
    "node_id": "fp_B23Lm!(Îž)",
    "feature_vec": [0.88, 0.21, 0.34, 0.50]
  }
]

//
[
  [0, 1],
  [1, 0]
]

//
import torch
from torch_geometric.data import Data
from typing import List, Tuple, Dict

def load_nodes_embeddings(node_meta: List[Dict]):
    ids = [m["node_id"] for m in node_meta]
    feats = torch.tensor([m["feature_vec"] for m in node_meta], dtype=torch.float)
    return feats, ids

def build_edge_index(edge_list: List[Tuple[int,int]]) -> torch.Tensor:
    return torch.tensor(edge_list, dtype=torch.long).t().contiguous()

def build_graph(node_meta: List[Dict], edge_list: List[Tuple[int,int]]) -> Data:
    x, ids = load_nodes_embeddings(node_meta)
    edge_index = build_edge_index(edge_list)
    data = Data(x=x, edge_index=edge_index)
    data.node_ids = ids
    return data

//
import torch
from torch_geometric.nn import GraphSAGE

class FingerprintGNN(torch.nn.Module):
    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, dropout=0.2):
        super().__init__()
        self.model = GraphSAGE(
            in_channels=in_channels,
            hidden_channels=hidden_channels,
            num_layers=num_layers,
            out_channels=out_channels,
            dropout=dropout
        )

    def forward(self, x, edge_index):
        return self.model(x, edge_index)

//

//
import torch
import torch.nn.functional as F
from torch_geometric.loader import NeighborLoader
from src.model import FingerprintGNN
from src.build_graph import build_graph
import json

def load_meta(nodes_path, edges_path):
    with open(nodes_path) as f1, open(edges_path) as f2:
        return json.load(f1), json.load(f2)

def main():
    node_meta, edge_list = load_meta("data/nodes.json", "data/edges.json")
    data = build_graph(node_meta, edge_list)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = FingerprintGNN(data.x.size(1), 64, 2).to(device)
    data = data.to(device)
    model.train()
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    loader = NeighborLoader(data, batch_size=2, num_neighbors=[2], shuffle=True)

    for epoch in range(10):
        total_loss = 0
        for batch in loader:
            optimizer.zero_grad()
            out = model(batch.x, batch.edge_index)
            loss = torch.mean(out)  # Placeholder: no labels
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        print(f"Epoch {epoch}: Loss {total_loss:.4f}")

if __name__ == "__main__":
    main()

//

//
import torch
from src.model import FingerprintGNN
from src.build_graph import build_graph
import json

def run_inference(nodes_path, edges_path, model_path):
    with open(nodes_path) as fn, open(edges_path) as fe:
        node_meta, edge_list = json.load(fn), json.load(fe)
    data = build_graph(node_meta, edge_list)
    model = FingerprintGNN(data.x.size(1), 64, 64)
    model.load_state_dict(torch.load(model_path))
    model.eval()
    with torch.no_grad():
        out = model(data.x, data.edge_index)
    return dict(zip(data.node_ids, out.tolist()))

if __name__ == "__main__":
    embs = run_inference("data/nodes.json", "data/edges.json", "model.pt")
    for k, v in embs.items():
        print(f"{k}: {v[:5]}...")

//
from src.model import FingerprintGNN

def test_model():
    model = FingerprintGNN(3, 8, 2)
    assert model is not None

//
import json
from pathlib import Path
from hash_engine.hasher import generate_fingerprint
from hash_engine.novelty import compute_novelty_score, link_lineage
from src.build_graph import build_graph
from src.infer import run_inference

EMBED_THRESHOLD = 0.8
NODE_FILE = Path("data/nodes.json")
EDGE_FILE = Path("data/edges.json")
MODEL_FILE = Path("model.pt")

def maybe_embed(content_block: str, cuid: str, uuid: str, shc: str = "Î»", parent_fp: str = None):
    fingerprint = generate_fingerprint(shc, cuid, uuid)
    novelty = compute_novelty_score(fingerprint)

    if novelty < EMBED_THRESHOLD:
        print(f"Skipped embedding (novelty={novelty:.3f}): {fingerprint}")
        return None

    # Link lineage if present
    if parent_fp:
        link_lineage(fingerprint, parent_fp)

    feature_vec = extract_features(content_block)
    append_node(fingerprint, feature_vec)
    return fingerprint

def extract_features(text: str):
    # Naive embedding: sum char ords normalized â€” REPLACE with Phi-3/LLM later
    base = [ord(c) for c in text if ord(c) < 256]
    padded = base[:64] + [0] * (64 - len(base[:64]))
    norm = [x / 255.0 for x in padded]
    return norm

def append_node(node_id: str, feature_vec: list):
    NODE_FILE.parent.mkdir(parents=True, exist_ok=True)
    if NODE_FILE.exists():
        nodes = json.loads(NODE_FILE.read_text())
    else:
        nodes = []
    nodes.append({"node_id": node_id, "feature_vec": feature_vec})
    NODE_FILE.write_text(json.dumps(nodes, indent=2))

def append_edge(idx1: int, idx2: int):
    EDGE_FILE.parent.mkdir(parents=True, exist_ok=True)
    if EDGE_FILE.exists():
        edges = json.loads(EDGE_FILE.read_text())
    else:
        edges = []
    edges.append([idx1, idx2])
    EDGE_FILE.write_text(json.dumps(edges, indent=2))

def embed_and_emit():
    embs = run_inference(str(NODE_FILE), str(EDGE_FILE), str(MODEL_FILE))
    for k, v in embs.items():
        print(f"[PUBSUB] fingerprint.embedded {k} â†’ {v[:5]}")
        # â†’ Replace with actual pub/sub emit: TAPS or internal bus


//
from ctas_gnn_bridge import maybe_embed

fp = maybe_embed(content_block=doc, cuid="docs/alpha.md", uuid="ALPHA-v1-SEC1")

//
python -m ctas_gnn_bridge embed_and_emit

//
// Rust hook from CTAS-7
HookType::PostCreate => {
    if doc.is_ip_relevant() {
        Command::new("python")
            .args(["-m", "ctas_gnn_bridge", "--embed", doc.path])
            .spawn()?;
    }
}

//

//

//

//

//

//

//


